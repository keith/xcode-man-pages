<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   -*- nroff -*-
 -->
<head>

<style>
@media (prefers-color-scheme: dark) {
  body {
    background: #000;
    color: #d0d0d0;
  }

  a, a:visited {
    color: #1899eb;
  }
}
</style>

  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    .Nd, .Bf, .Op { display: inline; }
    .Pa, .Ad { font-style: italic; }
    .Ms { font-weight: bold; }
    .Bl-diag > dt { font-weight: bold; }
    code.Nm, .Fl, .Cm, .Ic, code.In, .Fd, .Fn, .Cd { font-weight: bold;
      font-family: inherit; }
  </style>
  <title>MPSCNNConvolutionTranspose(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">MPSCNNConvolutionTranspose(3)</td>
    <td class="head-vol">MetalPerformanceShaders.framework</td>
    <td class="head-rtitle">MPSCNNConvolutionTranspose(3)</td>
  </tr>
</table>
<div class="manual-text">
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">MPSCNNConvolutionTranspose</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<p class="Pp">#import &lt;MPSCNNConvolution.h&gt;</p>
<p class="Pp">Inherits <b>MPSCNNKernel</b>.</p>
<section class="Ss">
<h2 class="Ss" id="Instance_Methods"><a class="permalink" href="#Instance_Methods">Instance
  Methods</a></h2>
<br/>
<p class="Pp">(nonnull instancetype) - <b>initWithDevice:weights:</b>
  <br/>
  (nonnull instancetype) - <b>initWithDevice:</b>
  <br/>
  (nullable instancetype) - <b>initWithCoder:device:</b>
  <br/>
  (<b>MPSImage</b> *__nonnull) -
    <b>encodeToCommandBuffer:sourceImage:convolutionGradientState:</b>
  <br/>
  (<b>MPSImageBatch</b> *__nonnull) -
    <b>encodeBatchToCommandBuffer:sourceImages:convolutionGradientStates:</b>
  <br/>
  (void) -
    <b>encodeToCommandBuffer:sourceImage:convolutionGradientState:destinationImage:</b>
  <br/>
  (void) -
    <b>encodeBatchToCommandBuffer:sourceImages:convolutionGradientStates:destinationImages:</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Properties"><a class="permalink" href="#Properties">Properties</a></h2>
<br/>
<p class="Pp">NSUInteger <b>inputFeatureChannels</b>
  <br/>
  NSUInteger <b>outputFeatureChannels</b>
  <br/>
  NSInteger <b>kernelOffsetX</b>
  <br/>
  NSInteger <b>kernelOffsetY</b>
  <br/>
  NSUInteger <b>groups</b>
  <br/>
  <b>MPSNNConvolutionAccumulatorPrecisionOption</b>
    <b>accumulatorPrecisionOption</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Additional_Inherited_Members"><a class="permalink" href="#Additional_Inherited_Members">Additional
  Inherited Members</a></h2>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Detailed_Description"><a class="permalink" href="#Detailed_Description">Detailed
  Description</a></h1>
<p class="Pp">This depends on Metal.framework The
    <b>MPSCNNConvolutionTranspose</b> specifies a transposed convolution. The
    <b>MPSCNNConvolutionTranspose</b> convolves the input image with a set of
    filters, each producing one feature map in the output image.</p>
<p class="Pp">Some third-party frameworks may rotate the weights spatially by
    180 degrees for Convolution Transpose. MPS uses the weights specified by the
    developer as-is and does not perform any rotation. The developer may need to
    rotate the weights appropriately in case this rotation is needed before the
    convolution transpose is applied.</p>
<p class="Pp">When the stride in any dimension is greater than 1, the
    convolution transpose puts (stride - 1) zeroes in-between the source image
    pixels to create an expanded image. Then a convolution is done over the
    expanded image to generate the output of the convolution transpose.</p>
<p class="Pp">Intermediate image size = (srcSize - 1) * Stride + 1</p>
<p class="Pp">Examples:</p>
<p class="Pp"></p>
<pre>So in case of sride == 2 (this behaves same in both dimensions)
Source image:
<br/>
 _______________
|   |   |   |   |
| 1 | 2 | 3 | 4 |
|   |   |   |   |
<br/>
 ---------------
Intermediate Image:
<br/>
 ___________________________
|   |   |   |   |   |   |   |
| 1 | 0 | 2 | 0 | 3 | 0 | 4 |
|   |   |   |   |   |   |   |
<br/>
 ---------------------------
NOTE on Offset:
There are 2 types of offsets defined:
1) The Offset defined in MPSCNNKernel from which MPSCNNConvolutionTranspose inherits. This offset is applied to from where
<br/>
   the kernel will be applied on the source.
2) The kernelOffsetX and kernelOffsetY which is the offset applied to the kernel when it is finally applied on the intermediate
<br/>
   image.
So totalOffset = Offset * stride + kernelOffset
The offset defined by user refers to the coordinate frame of the expanded image
(we are showing only 1 dimension X it can be extended to Y dimension as well) :
X indicates where the convolution transpose begins:
Intermediate Image:  Offset = 0, kernelOffset = 0
<br/>
 ___________________________
|   |   |   |   |   |   |   |
| 1 | 0 | 2 | 0 | 3 | 0 | 4 |
| X |   |   |   |   |   |   |
<br/>
 ---------------------------
X indicates where the convolution transpose begins:
Intermediate Image:  Offset = 0, kernelOffset = 1
<br/>
 ___________________________
|   |   |   |   |   |   |   |
| 1 | 0 | 2 | 0 | 3 | 0 | 4 |
|   | X |   |   |   |   |   |
<br/>
 ---------------------------
X indicates where the convolution transpose begins:
Intermediate Image:  Offset = 0, kernelOffset = -1
<br/>
   ___________________________
<br/>
  |   |   |   |   |   |   |   |
X | 1 | 0 | 2 | 0 | 3 | 0 | 4 |
<br/>
  |   |   |   |   |   |   |   |
<br/>
   ---------------------------
So if the user wanted to apply an offset of 2 on the source image of convolution transpose:
Source image:
<br/>
 _______________
|   |   |   |   |
| 1 | 2 | 3 | 4 |
|   |   | X |   |
<br/>
 ---------------
offset = 2, kernelOffset = 0
Intermediate Image:
<br/>
 ___________________________
|   |   |   |   |   |   |   |
| 1 | 0 | 2 | 0 | 3 | 0 | 4 |
|   |   |   |   | X |   |   |
<br/>
 ---------------------------</pre>
</section>
<section class="Sh">
<h1 class="Sh" id="Method_Documentation"><a class="permalink" href="#Method_Documentation">Method
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImageBatch</b> * __nonnull) encodeBatchToCommandBuffer:
  (nonnull id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b>
  *__nonnull) sourceImage(<b>MPSCNNConvolutionGradientStateBatch</b>
  *__nullable) convolutionGradientState</h2>
</section>
<section class="Ss">
<h2 class="Ss">- (void) encodeBatchToCommandBuffer: (nonnull id&lt;
  MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b> *__nonnull)
  sourceImage(<b>MPSCNNConvolutionGradientStateBatch</b> *__nullable)
  convolutionGradientState(<b>MPSImageBatch</b> *__nonnull)
  destinationImage</h2>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImage</b> * __nonnull) encodeToCommandBuffer: (nonnull
  id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImage</b> *__nonnull)
  sourceImage(<b>MPSCNNConvolutionGradientState</b> *__nullable)
  convolutionGradientState</h2>
<p class="Pp">Encode a <b>MPSCNNKernel</b> into a command Buffer. Create a
    texture to hold the result and return it. In the first iteration on this
    method, encodeToCommandBuffer:sourceImage:destinationImage: some work was
    left for the developer to do in the form of correctly setting the offset
    property and sizing the result buffer. With the introduction of the padding
    policy (see padding property) the filter can do this work itself. If you
    would like to have some input into what sort of <b>MPSImage</b> (e.g.
    temporary vs. regular) or what size it is or where it is allocated, you may
    set the destinationImageAllocator to allocate the image yourself.</p>
<p class="Pp">This method uses the <b>MPSNNPadding</b> padding property to
    figure out how to size the result image and to set the offset property. See
    discussion in <b>MPSNeuralNetworkTypes.h</b>.</p>
<p class="Pp">Note: the regular encodeToCommandBuffer:sourceImage: method may be
    used when no state is needed, such as when the convolution transpose
    operation is not balanced by a matching convolution object upstream.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The command buffer
<br/>
<i>sourceImage</i> <b>A</b> <b>MPSImage</b> to use as the source images for the
  filter.
<br/>
<i>convolutionGradientState</i> <b>A</b> valid
  <b>MPSCNNConvolutionGradientState</b> from the MPSCNNConvoluton counterpart to
  this <b>MPSCNNConvolutionTranspose</b>. If there is no forward convolution
  counterpart, pass NULL here. This state affects the sizing the result.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> <b>MPSImage</b> or <b>MPSTemporaryImage</b>
  allocated per the destinationImageAllocator containing the output of the
  graph. The offset property will be adjusted to reflect the offset used during
  the encode. The returned image will be automatically released when the command
  buffer completes. If you want to keep it around for longer, retain the image.
  (ARC will do this for you if you use it later.)</div>
</section>
<section class="Ss">
<h2 class="Ss">- (void) encodeToCommandBuffer: (nonnull id&lt; MTLCommandBuffer
  &gt;) commandBuffer(<b>MPSImage</b> *__nonnull)
  sourceImage(<b>MPSCNNConvolutionGradientState</b> *__nullable)
  convolutionGradientState(<b>MPSImage</b> *__nonnull) destinationImage</h2>
</section>
<section class="Ss">
<h2 class="Ss">- (nullable instancetype) <b>initWithCoder:</b> (NSCoder
  *__nonnull) aDecoder(nonnull id&lt; MTLDevice &gt;) device</h2>
<p class="Pp">&lt;NSSecureCoding&gt; support</p>
<p class="Pp">Reimplemented from <b>MPSCNNKernel</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype) initWithDevice: (nonnull id&lt;
  MTLDevice &gt;) device</h2>
<p class="Pp">Standard init with default properties per filter type</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The device that the filter will be used on.
  May not be NULL.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> pointer to the newly initialized object. This
  will fail, returning nil if the device is not supported. Devices must be
  MTLFeatureSet_iOS_GPUFamily2_v1 or later.</div>
<p class="Pp">Reimplemented from <b>MPSCNNKernel</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype) <b>initWithDevice:</b> (nonnull id&lt;
  MTLDevice &gt;) device(nonnull id&lt; <b>MPSCNNConvolutionDataSource</b> &gt;)
  weights</h2>
<p class="Pp">Initializes a convolution transpose kernel</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The MTLDevice on which this
  <b>MPSCNNConvolutionTranspose</b> filter will be used
<br/>
<i>weights</i> <b>A</b> pointer to a object that conforms to the
  <b>MPSCNNConvolutionDataSource</b> protocol. The
  <b>MPSCNNConvolutionDataSource</b> protocol declares the methods that an
  instance of <b>MPSCNNConvolutionTranspose</b> uses to obtain the weights and
  bias terms for the CNN convolutionTranspose filter. Currently we support only
  Float32 weights.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> valid <b>MPSCNNConvolutionTranspose</b>
  object.</div>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Property_Documentation"><a class="permalink" href="#Property_Documentation">Property
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (<b>MPSNNConvolutionAccumulatorPrecisionOption</b>)
  accumulatorPrecisionOption [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">Precision of accumulator used in convolution. See
    <b>MPSNeuralNetworkTypes.h</b> for discussion. Default is
    MPSNNConvolutionAccumulatorPrecisionOptionFloat.</p>
</section>
<section class="Ss">
<h2 class="Ss">- groups [read]<b>, [nonatomic]</b>, [assign]<b></b></h2>
<p class="Pp">Number of groups input and output channels are divided into.</p>
</section>
<section class="Ss">
<h2 class="Ss">- inputFeatureChannels [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">The number of feature channels per pixel in the input image.</p>
</section>
<section class="Ss">
<h2 class="Ss">- kernelOffsetX [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">Offset in X from which the kernel starts sliding</p>
</section>
<section class="Ss">
<h2 class="Ss">- kernelOffsetY [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">Offset in Y from which the kernel starts sliding</p>
</section>
<section class="Ss">
<h2 class="Ss">- outputFeatureChannels [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">The number of feature channels per pixel in the output image.</p>
<p class="Pp"></p>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Author"><a class="permalink" href="#Author">Author</a></h1>
<p class="Pp">Generated automatically by Doxygen for
    MetalPerformanceShaders.framework from the source code.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">Mon Jul 9 2018</td>
    <td class="foot-os">Version MetalPerformanceShaders-119.3</td>
  </tr>
</table>
</body>
</html>
