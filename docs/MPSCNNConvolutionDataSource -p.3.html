<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   -*- nroff -*-
 -->
<head>

<style>
@media (prefers-color-scheme: dark) {
  body {
    background: #000;
    color: #d0d0d0;
  }

  a, a:visited {
    color: #1899eb;
  }
}
</style>

  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    .Nd, .Bf, .Op { display: inline; }
    .Pa, .Ad { font-style: italic; }
    .Ms { font-weight: bold; }
    .Bl-diag > dt { font-weight: bold; }
    code.Nm, .Fl, .Cm, .Ic, code.In, .Fd, .Fn, .Cd { font-weight: bold;
      font-family: inherit; }
  </style>
  <title>&lt;MPSCNNConvolutionDataSource &gt;(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">&lt;MPSCNNConvolutionDataSource &gt;(3)</td>
    <td class="head-vol">MetalPerformanceShaders.framework</td>
    <td class="head-rtitle">&lt;MPSCNNConvolutionDataSource &gt;(3)</td>
  </tr>
</table>
<div class="manual-text">
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">&lt;MPSCNNConvolutionDataSource &gt;</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<p class="Pp">#import &lt;MPSCNNConvolution.h&gt;</p>
<p class="Pp">Inherits &lt;MPSStateNSCopying&gt;, and &lt;NSObject&gt;.</p>
<section class="Ss">
<h2 class="Ss" id="Instance_Methods"><a class="permalink" href="#Instance_Methods">Instance
  Methods</a></h2>
<br/>
<p class="Pp">(<b>MPSDataType</b>) - <b>dataType</b>
  <br/>
  (<b>MPSCNNConvolutionDescriptor</b> *__nonnull) - <b>descriptor</b>
  <br/>
  (void *__nonnull) - <b>weights</b>
  <br/>
  (float *__nullable) - <b>biasTerms</b>
  <br/>
  (BOOL) - <b>load</b>
  <br/>
  (void) - <b>purge</b>
  <br/>
  (NSString *__nullable) - <b>label</b>
  <br/>
  (vector_float2 *__nonnull) - <b>rangesForUInt8Kernel</b>
  <br/>
  (float *__nonnull) - <b>lookupTableForUInt8Kernel</b>
  <br/>
  (<b>MPSCNNWeightsQuantizationType</b>) - <b>weightsQuantizationType</b>
  <br/>
  (<b>MPSCNNConvolutionWeightsAndBiasesState</b> *__nullable) -
    <b>updateWithCommandBuffer:gradientState:sourceState:</b>
  <br/>
  (BOOL) - <b>updateWithGradientState:sourceState:</b>
  <br/>
  (nonnull instancetype) - <b>copyWithZone:device:</b>
  <br/>
  <br/>
</p>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Method_Documentation"><a class="permalink" href="#Method_Documentation">Method
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (float * __nullable <b>MPSCNNConvolutionDataSource</b>)
  biasTerms [required]<b></b></h2>
<p class="Pp">Returns a pointer to the bias terms for the convolution. Each
    entry in the array is a single precision IEEE-754 float and represents one
    bias. The number of entries is equal to outputFeatureChannels.</p>
<p class="Pp">Frequently, this function is a single line of code to return a
    pointer to memory allocated in -load. It may also just return nil.</p>
<p class="Pp">Note: bias terms are always float, even when the weights are
  not.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype <b>MPSCNNConvolutionDataSource</b>)
  copyWithZone: (nullable NSZone *) zone(nullable id&lt; MTLDevice &gt;) device
  [optional]<b></b></h2>
<p class="Pp">When copyWithZone:device on convolution is called, data source
    copyWithZone:device will be called if data source object responds to this
    selector. If not, copyWithZone: will be called if data source responds to
    it. Otherwise, it is simply retained. This is to allow application to make a
    separate copy of data source in convolution when convolution itself is
    coplied, for example when copying training graph for running on second GPU
    so that weights update on two different GPUs dont end up stomping same data
    source.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSDataType</b> <b>MPSCNNConvolutionDataSource</b>)
  dataType [required]<b></b></h2>
<p class="Pp">Alerts MPS what sort of weights are provided by the object For
    <b>MPSCNNConvolution</b>, MPSDataTypeUInt8, MPSDataTypeFloat16 and
    MPSDataTypeFloat32 are supported for normal convolutions using
    <b>MPSCNNConvolution</b>. <b>MPSCNNBinaryConvolution</b> assumes weights to
    be of type MPSDataTypeUInt32 always.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSCNNConvolutionDescriptor</b> * __nonnull
  <b>MPSCNNConvolutionDataSource</b>) descriptor [required]<b></b></h2>
<p class="Pp">Return a <b>MPSCNNConvolutionDescriptor</b> as needed MPS will not
    modify this object other than perhaps to retain it. User should set the
    appropriate neuron in the creation of convolution descriptor and for batch
    normalization use:</p>
<p class="Pp"></p>
<pre>-setBatchNormalizationParametersForInferenceWithMean:variance:gamma:beta:epsilon:
</pre>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> <b>MPSCNNConvolutionDescriptor</b> that
  describes the kernel housed by this object.</div>
</section>
<section class="Ss">
<h2 class="Ss">- (NSString*__nullable <b>MPSCNNConvolutionDataSource</b>) label
  [required]<b></b></h2>
<p class="Pp"><b>A</b> label that is transferred to the convolution at init time
    Overridden by a <b>MPSCNNConvolutionNode.label</b> if it is non-nil.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (BOOL <b>MPSCNNConvolutionDataSource</b>) load
  [required]<b></b></h2>
<p class="Pp">Alerts the data source that the data will be needed soon Each load
    alert will be balanced by a purge later, when MPS no longer needs the data
    from this object. Load will always be called atleast once after initial
    construction or each purge of the object before anything else is called.
    Note: load may be called to merely inspect the descriptor. In some
    circumstances, it may be worthwhile to postpone weight and bias construction
    until they are actually needed to save touching memory and keep the working
    set small. The load function is intended to be an opportunity to open files
    or mark memory no longer purgeable.</p>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">Returns YES on success. If NO is returned, expect MPS
  object construction to fail.</div>
</section>
<section class="Ss">
<h2 class="Ss">- (float * __nonnull <b>MPSCNNConvolutionDataSource</b>)
  lookupTableForUInt8Kernel [optional]<b></b></h2>
<p class="Pp"><b>A</b> pointer to a 256 entry lookup table containing the values
    to use for the weight range [0,255]</p>
</section>
<section class="Ss">
<h2 class="Ss">- (void <b>MPSCNNConvolutionDataSource</b>) purge
  [required]<b></b></h2>
<p class="Pp">Alerts the data source that the data is no longer needed Each load
    alert will be balanced by a purge later, when MPS no longer needs the data
    from this object.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (vector_float2 * __nonnull <b>MPSCNNConvolutionDataSource</b>)
  rangesForUInt8Kernel [optional]<b></b></h2>
<p class="Pp"><b>A</b> list of per-output channel limits that describe the 8-bit
    range This returns a pointer to an array of
    vector_float2[outputChannelCount] values. The first value in the vector is
    the minimum value in the range. The second value in the vector is the
    maximum value in the range.</p>
<p class="Pp">The 8-bit weight value is interpreted as:</p>
<p class="Pp"></p>
<pre>float unorm8_weight = uint8_weight / 255.0f;    // unorm8_weight has range [0,1.0]
float max = range[outputChannel].y;
float min = range[outputChannel].x;
float weight = unorm8_weight * (max - min) + min
</pre>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSCNNConvolutionWeightsAndBiasesState</b>* __nullable
  <b>MPSCNNConvolutionDataSource</b>) updateWithCommandBuffer: (__nonnull id&lt;
  MTLCommandBuffer &gt;) commandBuffer(<b>MPSCNNConvolutionGradientState</b>
  *__nonnull) gradientState(<b>MPSCNNConvolutionWeightsAndBiasesState</b>
  *__nonnull) sourceState [optional]<b></b></h2>
<p class="Pp">Callback for the <b>MPSNNGraph</b> to update the convolution
    weights on GPU. It is the resposibility of this method to decrement the read
    count of both the gradientState and the sourceState before returning. BUG:
    prior to macOS 10.14, ios/tvos 12.0, the <b>MPSNNGraph</b> incorrectly
    decrements the readcount of the gradientState after this method is
  called.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The command buffer on which to do
  the update. MPSCNNConvolutionGradientNode.MPSNNTrainingStyle controls where
  you want your update to happen. Provide implementation of this function for
  GPU side update.
<br/>
<i>gradientState</i> <b>A</b> state object produced by the
  <b>MPSCNNConvolution</b> and updated by <b>MPSCNNConvolutionGradient</b>
  containing weight gradients.
<br/>
<i>sourceState</i> <b>A</b> state object containing the convolution
  weights</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">If NULL, no update occurs. If nonnull, the result will be
  used to update the weights in the <b>MPSNNGraph</b></div>
</section>
<section class="Ss">
<h2 class="Ss">- (BOOL <b>MPSCNNConvolutionDataSource</b>)
  updateWithGradientState: (<b>MPSCNNConvolutionGradientState</b> *__nonnull)
  gradientState(<b>MPSCNNConvolutionWeightsAndBiasesState</b> *__nonnull)
  sourceState [optional]<b></b></h2>
<p class="Pp">Callback for the <b>MPSNNGraph</b> to update the convolution
    weights on CPU. MPSCNNConvolutionGradientNode.MPSNNTrainingStyle controls
    where you want your update to happen. Provide implementation of this
    function for CPU side update.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>gradientState</i> <b>A</b> state object produced by
  the <b>MPSCNNConvolution</b> and updated by <b>MPSCNNConvolutionGradient</b>
  containing weight gradients. <b>MPSNNGraph</b> is responsible for calling
  [gradientState synchronizeOnCommandBuffer:] so that application get correct
  gradients for CPU side update.
<br/>
<i>sourceState</i> <b>A</b> state object containing the convolution weights
  used. <b>MPSCNNConvolution</b> and <b>MPSCNNConvolutionGradient</b>
  reloadWeightsWithDataSource will be called right after this method is called.
  Note that the weights returned here may not match the weights in your data
  source due to conversion loss. These are the weights actually used, and should
  be what you use to calculate the new weights. Your copy may be incorrect.
  Write the new weights to your copy and return them out the left hand
  side.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">TRUE if success/no error, FALSE in case of failure.</div>
</section>
<section class="Ss">
<h2 class="Ss">- (void * __nonnull <b>MPSCNNConvolutionDataSource</b>) weights
  [required]<b></b></h2>
<p class="Pp">Returns a pointer to the weights for the convolution. The type of
    each entry in array is given by -dataType. The number of entries is equal
    to:</p>
<p class="Pp"></p>
<pre>inputFeatureChannels * outputFeatureChannels * kernelHeight * kernelWidth
</pre>
<p class="Pp">
  <br/>
   The layout of filter weight is as a 4D tensor (array) weight[ outputChannels
    ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]</p>
<p class="Pp">Frequently, this function is a single line of code to return a
    pointer to memory allocated in -load.</p>
<p class="Pp">Batch normalization parameters are set using -descriptor.</p>
<p class="Pp">Note: For binary-convolutions the layout of the weights are:
    weight[ outputChannels ][ kernelHeight ][ kernelWidth ][
    floor((inputChannels/groups)+31) / 32 ] with each 32 sub input feature
    channel index specified in machine byte order, so that for example the 13th
    feature channel bit can be extracted using bitmask = (1U &lt;&lt; 13).</p>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSCNNWeightsQuantizationType</b>
  <b>MPSCNNConvolutionDataSource</b>) weightsQuantizationType
  [optional]<b></b></h2>
<p class="Pp">Quantizaiton type of weights. If it returns
    MPSCNNWeightsQuantizationTypeLookupTable, lookupTableForUInt8Kernel method
    must be implmented. if it returns MPSCNNWeightsQuantizationTypeLookupLinear,
    rangesForUInt8Kernel method must be implemented.</p>
<p class="Pp"></p>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Author"><a class="permalink" href="#Author">Author</a></h1>
<p class="Pp">Generated automatically by Doxygen for
    MetalPerformanceShaders.framework from the source code.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">Mon Jul 9 2018</td>
    <td class="foot-os">Version MetalPerformanceShaders-119.3</td>
  </tr>
</table>
</body>
</html>
