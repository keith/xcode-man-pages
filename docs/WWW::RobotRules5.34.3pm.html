<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   Automatically generated by Pod::Man 4.14 (Pod::Simple 3.42)
  
   Standard preamble:
   ========================================================================
   Vertical space (when we can't use .PP)
 -->
<head>

<style>
@media (prefers-color-scheme: dark) {
  body {
    background: #000;
    color: #d0d0d0;
  }

  a, a:visited {
    color: #1899eb;
  }
}
</style>

  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    .Nd, .Bf, .Op { display: inline; }
    .Pa, .Ad { font-style: italic; }
    .Ms { font-weight: bold; }
    .Bl-diag > dt { font-weight: bold; }
    code.Nm, .Fl, .Cm, .Ic, code.In, .Fd, .Fn, .Cd { font-weight: bold;
      font-family: inherit; }
  </style>
  <title>WWW::RobotRules(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">WWW::RobotRules(3)</td>
    <td class="head-vol">User Contributed Perl Documentation</td>
    <td class="head-rtitle">WWW::RobotRules(3)</td>
  </tr>
</table>
<div class="manual-text">
<br/>
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">WWW::RobotRules - database of robots.txt-derived permissions</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<pre> use WWW::RobotRules;
 my $rules = WWW::RobotRules-&gt;new('MOMspider/1.0');
 use LWP::Simple qw(get);
 {
   my $url = &quot;http://some.place/robots.txt&quot;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }
 {
   my $url = &quot;http://some.other.place/robots.txt&quot;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }
 # Now we can check if a URL is valid for those servers
 # whose &quot;robots.txt&quot; files we've gotten and parsed:
 if($rules-&gt;allowed($url)) {
     $c = get $url;
     ...
 }
</pre>
</section>
<section class="Sh">
<h1 class="Sh" id="DESCRIPTION"><a class="permalink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<p class="Pp">This module parses <i>/robots.txt</i> files as specified in
    &quot;A Standard for Robot Exclusion&quot;, at
    &lt;http://www.robotstxt.org/wc/norobots.html&gt; Webmasters can use the
    <i>/robots.txt</i> file to forbid conforming robots from accessing parts of
    their web site.</p>
<p class="Pp">The parsed files are kept in a WWW::RobotRules object, and this
    object provides methods to check if access to a given URL is prohibited. The
    same WWW::RobotRules object can be used for one or more parsed
    <i>/robots.txt</i> files on any number of hosts.</p>
<p class="Pp">The following methods are provided:</p>
<dl class="Bl-tag">
  <dt>$rules = WWW::RobotRules-&gt;new($robot_name)</dt>
  <dd>This is the constructor for WWW::RobotRules objects. The first argument
      given to <b>new()</b> is the name of the robot.</dd>
  <dt>$rules-&gt;parse($robot_txt_url, $content, $fresh_until)</dt>
  <dd>The <b>parse()</b> method takes as arguments the URL that was used to
      retrieve the <i>/robots.txt</i> file, and the contents of the file.</dd>
  <dt>$rules-&gt;allowed($uri)</dt>
  <dd>Returns TRUE if this robot is allowed to retrieve this URL.</dd>
  <dt>$rules-&gt;agent([$name])</dt>
  <dd>Get/set the agent name. NOTE: Changing the agent name will clear the
      robots.txt rules and expire times out of the cache.</dd>
</dl>
</section>
<section class="Sh">
<h1 class="Sh" id="ROBOTS.TXT"><a class="permalink" href="#ROBOTS.TXT">ROBOTS.TXT</a></h1>
<p class="Pp">The format and semantics of the &quot;/robots.txt&quot; file are
    as follows (this is an edited abstract of
    &lt;http://www.robotstxt.org/wc/norobots.html&gt;):</p>
<p class="Pp">The file consists of one or more records separated by one or more
    blank lines. Each record contains lines of the form</p>
<p class="Pp"></p>
<pre>  &lt;field-name&gt;: &lt;value&gt;
</pre>
<p class="Pp">The field name is case insensitive. Text after the '#' character
    on a line is ignored during parsing. This is used for comments. The
    following &lt;field-names&gt; can be used:</p>
<dl class="Bl-tag">
  <dt id="User-Agent"><a class="permalink" href="#User-Agent">User-Agent</a></dt>
  <dd>The value of this field is the name of the robot the record is describing
      access policy for. If more than one <i>User-Agent</i> field is present the
      record describes an identical access policy for more than one robot. At
      least one field needs to be present per record. If the value is '*', the
      record describes the default access policy for any robot that has not not
      matched any of the other records.
    <p class="Pp">The <i>User-Agent</i> fields must occur before the
        <i>Disallow</i> fields. If a record contains a <i>User-Agent</i> field
        after a <i>Disallow</i> field, that constitutes a malformed record. This
        parser will assume that a blank line should have been placed before that
        <i>User-Agent</i> field, and will break the record into two. All the
        fields before the <i>User-Agent</i> field will constitute a record, and
        the <i>User-Agent</i> field will be the first field in a new record.</p>
  </dd>
  <dt id="Disallow"><a class="permalink" href="#Disallow">Disallow</a></dt>
  <dd>The value of this field specifies a partial URL that is not to be visited.
      This can be a full path, or a partial path; any URL that starts with this
      value will not be retrieved</dd>
</dl>
<p class="Pp">Unrecognized records are ignored.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="ROBOTS.TXT_EXAMPLES"><a class="permalink" href="#ROBOTS.TXT_EXAMPLES">ROBOTS.TXT
  EXAMPLES</a></h1>
<p class="Pp">The following example &quot;/robots.txt&quot; file specifies that
    no robots should visit any URL starting with &quot;/cyberworld/map/&quot; or
    &quot;/tmp/&quot;:</p>
<p class="Pp"></p>
<pre>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear
</pre>
<p class="Pp">This example &quot;/robots.txt&quot; file specifies that no robots
    should visit any URL starting with &quot;/cyberworld/map/&quot;, except the
    robot called &quot;cybermapper&quot;:</p>
<p class="Pp"></p>
<pre>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:
</pre>
<p class="Pp">This example indicates that no robots should visit this site
    further:</p>
<p class="Pp"></p>
<pre>  # go away
  User-agent: *
  Disallow: /
</pre>
<p class="Pp">This is an example of a malformed robots.txt file.</p>
<p class="Pp"></p>
<pre>  # robots.txt for ancientcastle.example.com
  # I've locked myself away.
  User-agent: *
  Disallow: /
  # The castle is your home now, so you can go anywhere you like.
  User-agent: Belle
  Disallow: /west-wing/ # except the west wing!
  # It's good to be the Prince...
  User-agent: Beast
  Disallow:
</pre>
<p class="Pp">This file is missing the required blank lines between records.
    However, the intention is clear.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SEE_ALSO"><a class="permalink" href="#SEE_ALSO">SEE
  ALSO</a></h1>
<p class="Pp">LWP::RobotUA, WWW::RobotRules::AnyDBM_File</p>
</section>
<section class="Sh">
<h1 class="Sh" id="COPYRIGHT"><a class="permalink" href="#COPYRIGHT">COPYRIGHT</a></h1>
<pre>  Copyright 1995-2009, Gisle Aas
  Copyright 1995, Martijn Koster
</pre>
<p class="Pp">This library is free software; you can redistribute it and/or
    modify it under the same terms as Perl itself.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">2012-02-18</td>
    <td class="foot-os">perl v5.34.0</td>
  </tr>
</table>
</body>
</html>
