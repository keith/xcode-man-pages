<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   -*- nroff -*-
 -->
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
    div.Nd, div.Bf, div.Op { display: inline; }
    span.Pa, span.Ad { font-style: italic; }
    span.Ms { font-weight: bold; }
    dl.Bl-diag > dt { font-weight: bold; }
    code.Nm, code.Fl, code.Cm, code.Ic, code.In, code.Fd, code.Fn,
    code.Cd { font-weight: bold; font-family: inherit; }
  </style>
  <title>MPSCNNLoss(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">MPSCNNLoss(3)</td>
    <td class="head-vol">MetalPerformanceShaders.framework</td>
    <td class="head-rtitle">MPSCNNLoss(3)</td>
  </tr>
</table>
<div class="manual-text">
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
MPSCNNLoss
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<p class="Pp">#import &lt;MPSCNNLoss.h&gt;</p>
<p class="Pp">Inherits <b>MPSCNNKernel</b>.</p>
<section class="Ss">
<h2 class="Ss" id="Instance_Methods"><a class="permalink" href="#Instance_Methods">Instance
  Methods</a></h2>
<br/>
(nonnull instancetype) - <b>initWithDevice:</b>
<br/>
(nonnull instancetype) - <b>initWithDevice:lossDescriptor:</b>
<br/>
(nullable instancetype) - <b>initWithCoder:device:</b>
<br/>
(void) - <b>encodeToCommandBuffer:sourceImage:labels:destinationImage:</b>
<br/>
(<b>MPSImage</b> *__nonnull) - <b>encodeToCommandBuffer:sourceImage:labels:</b>
<br/>
(void) -
  <b>encodeBatchToCommandBuffer:sourceImages:labels:destinationImages:</b>
<br/>
(<b>MPSImageBatch</b> *__nonnull) -
  <b>encodeBatchToCommandBuffer:sourceImages:labels:</b>
<br/>
<br/>
</section>
<section class="Ss">
<h2 class="Ss" id="Properties"><a class="permalink" href="#Properties">Properties</a></h2>
<br/>
<b>MPSCNNLossType</b> <b>lossType</b>
<br/>
<b>MPSCNNReductionType</b> <b>reductionType</b>
<br/>
float <b>weight</b>
<br/>
float <b>labelSmoothing</b>
<br/>
NSUInteger <b>numberOfClasses</b>
<br/>
float <b>epsilon</b>
<br/>
float <b>delta</b>
<br/>
<br/>
</section>
<section class="Ss">
<h2 class="Ss" id="Additional_Inherited_Members"><a class="permalink" href="#Additional_Inherited_Members">Additional
  Inherited Members</a></h2>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Detailed_Description"><a class="permalink" href="#Detailed_Description">Detailed
  Description</a></h1>
This depends on Metal.framework. The <b>MPSCNNLoss</b> filter is only used for
  training. This filter performs both the forward and backward pass
  computations. Specifically, it computes the loss between the input
  (predictions) and target data (labels) and the loss gradient. The loss value
  can be a 1 x 1 x 1 image containing a scalar loss value or an image (of the
  same size as the input source image) with per feature channel losses. The loss
  value is used to determine whether to continue the training operation or to
  terminate it, once satisfactory results are achieved. The loss gradient is the
  first gradient computed for the backward pass and serves as input to the next
  gradient filter (in the backward direction).
<p class="Pp">The <b>MPSCNNLoss</b> filter is created with a
    <b>MPSCNNLossDescriptor</b> describing the type of a loss filter and the
    type of a reduction to use for computing the overall loss.</p>
<p class="Pp">The <b>MPSCNNLoss</b> filter takes the output of the inference
    pass (predictions) as input. It also requires the target data (labels) and
    optionally, weights for the labels. If per-label weights are not supplied,
    there is an option to use a single weight value by setting the 'weight'
    properly on the <b>MPSCNNLossDescriptor</b> object. The labels and optional
    weights need to be supplied by the user using the <b>MPSCNNLossLabels</b>
    object. The labels and weights are described via the
    <b>MPSCNNLossDataDescriptor</b> objects, which are in turn used to
    initialize the <b>MPSCNNLossLabels</b> object.</p>
<p class="Pp">If the specified reduction operation is MPSCNNReductionTypeNone,
    the destinationImage should be at least as large as the specified clipRect.
    The detinationImage will then contain per-element losses. Otherse, a
    reduction operation will be performed, according to the specified reduction
    type, and the filter will return a scalar value containing the overall loss.
    For more information on the available reduction types, see
    <b>MPSCNNTypes.h</b>. Also see <b>MPSCNNLossDescriptor</b> for the
    description of optional parameters.</p>
<p class="Pp">Here is a code example:</p>
<p class="Pp">// Setup MPSCNNLossDataDescriptor* labelsDescriptor =
    [<b>MPSCNNLossDataDescriptor</b> cnnLossDataDescriptorWithData: labelsData
    layout: MPSDataLayoutHeightxWidthxFeatureChannels size: labelsDataSize];
    MPSCNNLossLabels* labels = [[<b>MPSCNNLossLabels</b> alloc] initWithDevice:
    device labelsDescriptor: labelsDescriptor]; <b>MPSCNNLossDescriptor</b>
    <i>lossDescriptor = [</i><b>MPSCNNLossDescriptor</b><i>
    cnnLossDescriptorWithType: (MPSCNNLossType)MPSCNNLossTypeMeanAbsoluteError
    reductionType: (MPSCNNReductionType)MPSCNNReductionTypeSum];
    </i><b>MPSCNNLoss</b><i></i><b> lossFilter = [[</b><b>MPSCNNLoss</b><b>
    alloc] initWithDevice: device lossDescriptor: lossDescriptor];</b></p>
<p class="Pp">// Encode loss filter. // The sourceImage is the output of a
    previous layer, for example, the SoftMax layer. The lossGradientsImage // is
    the sourceGradient input image to the first gradient layer (in the backward
    direction), for example, // the SoftMax gradient filter. [lossFilter
    encodeToCommandBuffer: commandBuffer sourceImage: sourceImage labels: labels
    destinationImage: lossGradientsImage];</p>
<p class="Pp">// In order to gaurantee that the loss image data is correctly
    synchronized for CPU side access, // it is the application's responsibility
    to call the [labels synchronizeOnCommandBuffer:] // method before accessing
    the loss image data. [labels synchronizeOnCommandBuffer:commandBuffer];
    MPSImage* lossImage = [labels lossImage];</p>
<p class="Pp"></p>
<pre>
        For predictions (y) and labels (t), the available loss filter types are the following:
        Mean Absolute Error loss filter. This filter measures the absolute error of the element-wise
        difference between the predictions and labels.
        This loss function is computed according to the following formulas:
            Compute losses:          losses = |y - t|
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        Mean Squared Error loss filter. This filter measures the squared error of the element-wise
        difference between the predictions and labels.
        This loss function is computed according to the following formulas:
            Compute losses:          losses = (y - t)^2
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        SoftMax Cross Entropy loss filter. This loss filter is applied element-wise.
        This loss filter combines the LogSoftMax and Negative Log Likelihood operations in a
        single filter. It is useful for training a classification problem with multiple classes.
        This loss function is computed according to the following formulas:
            Compute losses:          losses = -t * LogSoftMax(y)
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
                                     If reductionType is MPSCNNReductionTypeMean, the accumulated
                                     loss value is divided by width * height instead of
                                     width * height * featureChannels.
        Sigmoid Cross Entropy loss filter. This loss filter is applied element-wise.
        This loss function is computed according to the following formulas:
            Compute losses:          losses = max(y, 0) - y * t + log(1 + exp(-|y|))
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        Categorical Cross Entropy loss filter. This loss filter is applied element-wise.
        This loss function is computed according to the following formulas:
            Compute losses:          losses = -t * log(y)
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        Hinge loss filter. This loss filter is applied element-wise.
        The labels are expected to be 0.0 or 1.0.
        This loss function is computed according to the following formulas:
            Compute losses:          losses = max(1 - (t * y), 0.0f)
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        Huber loss filter. This loss filter is applied element-wise.
        This loss function is computed according to the following formulas:
            Compute losses:          if (|y - t| &lt;= delta, losses = 0.5 * y^2
                                     if (|y - t| &gt;  delta, losses = 0.5 * delta^2 + delta * (|y - t| - delta)
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        Cosine Distance loss filter. This loss filter is applied element-wise.
        The only valid reduction type for this loss filter is MPSCNNReductionTypeSum.
        This loss function is computed according to the following formulas:
            Compute losses:          loss = 1 - reduce_sum(y * t)
            Compute overall loss:    weighted_loss = weight * loss
        Log loss filter. This loss filter is applied element-wise.
        This loss function is computed according to the following formulas:
            Compute losses:          losses = -(t * log(y + epsilon)) - ((1 - t) * log(1 - y + epsilon))
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        Kullback-Leibler Divergence loss filter. This loss filter is applied element-wise.
        The input (predictions) is expected to contain log-probabilities.
            This loss function is computed according to the following formulas:
            Compute losses:          losses = t * (log(t) - y)
            Compute weighted losses: weighted_losses = weight(s) * losses
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
        For predictions (y) and labels (t), the loss gradient for each available loss filter type
        is computed as follows:
        Mean Absolute Error loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = (y - t) / |y - t|
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Mean Squared Error loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = 2 * (y - t)
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        SoftMax Cross Entropy loss.
        The loss gradient is computed according to the following formulas:
            First, apply the same label smoothing as in the MPSCNNLoss filter.
            Compute gradient:          d/dy = y - t
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Sigmoid Cross Entropy loss.
        The loss gradient is computed according to the following formulas:
        First, apply the same label smoothing as in the MPSCNNLoss filter.
            Compute gradient:          d/dy = (1 / (1 + exp(-y)) - t
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Categorical Cross Entropy loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = -t / y
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Hinge loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = ((1 + ((1 - (2 * t)) * y)) &gt; 0) ? 1 - (2 * t) : 0
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Huber loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = |y - t| &gt; delta ? delta : y - t
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Cosine Distance loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = -t
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Log loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = (-2 * epsilon * t - t + y + epsilon) / (y * (1 - y) + epsilon * (epsilon + 1))
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        Kullback-Leibler Divergence loss.
        The loss gradient is computed according to the following formulas:
            Compute gradient:          d/dy = -t / y
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
        The number of output feature channels remains the same as the number of input feature
        channels..fi
</pre>
<pre>
 
</pre>
</section>
<section class="Sh">
<h1 class="Sh" id="Method_Documentation"><a class="permalink" href="#Method_Documentation">Method
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss" id="_-_(_fBMPSImageBatch_fP*__nonnull)_encodeBatchToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImageBatch_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabelsBatch_fP_*__nonnull)_labels"><a class="permalink" href="#_-_(_fBMPSImageBatch_fP*__nonnull)_encodeBatchToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImageBatch_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabelsBatch_fP_*__nonnull)_labels">-
  (<b>MPSImageBatch</b>*__nonnull) encodeBatchToCommandBuffer: (nonnull id&lt;
  MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b> *__nonnull)
  sourceImage(<b>MPSCNNLossLabelsBatch</b> *__nonnull) labels</a></h2>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(void)_encodeBatchToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImageBatch_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabelsBatch_fP_*__nonnull)_labels(_fBMPSImageBatch_fP_*__nonnull)_destinationImage"><a class="permalink" href="#_-_(void)_encodeBatchToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImageBatch_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabelsBatch_fP_*__nonnull)_labels(_fBMPSImageBatch_fP_*__nonnull)_destinationImage">-
  (void) encodeBatchToCommandBuffer: (nonnull id&lt; MTLCommandBuffer &gt;)
  commandBuffer(<b>MPSImageBatch</b> *__nonnull)
  sourceImage(<b>MPSCNNLossLabelsBatch</b> *__nonnull)
  labels(<b>MPSImageBatch</b> *__nonnull) destinationImage</a></h2>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(_fBMPSImage_fP*__nonnull)_encodeToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImage_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabels_fP_*__nonnull)_labels"><a class="permalink" href="#_-_(_fBMPSImage_fP*__nonnull)_encodeToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImage_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabels_fP_*__nonnull)_labels">-
  (<b>MPSImage</b>*__nonnull) encodeToCommandBuffer: (nonnull id&lt;
  MTLCommandBuffer &gt;) commandBuffer(<b>MPSImage</b> *__nonnull)
  sourceImage(<b>MPSCNNLossLabels</b> *__nonnull) labels</a></h2>
Encode a <b>MPSCNNLoss</b> filter and return a gradient. This -encode call is
  similar to the encodeToCommandBuffer:sourceImage:labels:destinationImage:
  above, except that it creates and returns the <b>MPSImage</b> with the loss
  gradient result.
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The MTLCommandBuffer on which to
  encode.
<br/>
<i>sourceImage</i> The source image from the previous filter in the graph (in
  the inference direction).
<br/>
<i>labels</i> The object containing the target data (labels) and optionally,
  weights for the labels.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">The <b>MPSImage</b> containing the gradient result.</div>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(void)_encodeToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImage_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabels_fP_*__nonnull)_labels(_fBMPSImage_fP_*__nonnull)_destinationImage"><a class="permalink" href="#_-_(void)_encodeToCommandBuffer:_(nonnull_id__MTLCommandBuffer__)_commandBuffer(_fBMPSImage_fP_*__nonnull)_sourceImage(_fBMPSCNNLossLabels_fP_*__nonnull)_labels(_fBMPSImage_fP_*__nonnull)_destinationImage">-
  (void) encodeToCommandBuffer: (nonnull id&lt; MTLCommandBuffer &gt;)
  commandBuffer(<b>MPSImage</b> *__nonnull) sourceImage(<b>MPSCNNLossLabels</b>
  *__nonnull) labels(<b>MPSImage</b> *__nonnull) destinationImage</a></h2>
Encode a <b>MPSCNNLoss</b> filter and return a gradient in the destinationImage.
  This filter consumes the output of a previous layer, for example, the SoftMax
  layer containing predictions, and the <b>MPSCNNLossLabels</b> object
  containing the target data (labels) and optionally, weights for the labels.
  The destinationImage contains the computed gradient for the loss layer. It
  serves as a source gradient input image to the first gradient layer (in the
  backward direction), in our example, the SoftMax gradient layer.
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The MTLCommandBuffer on which to
  encode.
<br/>
<i>sourceImage</i> The source image from the previous filter in the graph (in
  the inference direction).
<br/>
<i>labels</i> The object containing the target data (labels) and optionally,
  weights for the labels.
<br/>
<i>destinationImage</i> The <b>MPSImage</b> into which to write the gradient
  result.</div>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(nullable_instancetype)__fBinitWithCoder:_fP_(NSCoder_*__nonnull)_aDecoder(nonnull_id__MTLDevice__)_device"><a class="permalink" href="#_-_(nullable_instancetype)__fBinitWithCoder:_fP_(NSCoder_*__nonnull)_aDecoder(nonnull_id__MTLDevice__)_device">-
  (nullable instancetype) <b>initWithCoder:</b> (NSCoder *__nonnull)
  aDecoder(nonnull id&lt; MTLDevice &gt;) device</a></h2>
&lt;NSSecureCoding&gt; support
<p class="Pp">Reimplemented from <b>MPSCNNKernel</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(nonnull_instancetype)_initWithDevice:_(nonnull_id__MTLDevice__)_device"><a class="permalink" href="#_-_(nonnull_instancetype)_initWithDevice:_(nonnull_id__MTLDevice__)_device">-
  (nonnull instancetype) initWithDevice: (nonnull id&lt; MTLDevice &gt;)
  device</a></h2>
Standard init with default properties per filter type
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The device that the filter will be used on.
  May not be NULL.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> pointer to the newly initialized object. This
  will fail, returning nil if the device is not supported. Devices must be
  MTLFeatureSet_iOS_GPUFamily2_v1 or later.</div>
<p class="Pp">Reimplemented from <b>MPSCNNKernel</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(nonnull_instancetype)__fBinitWithDevice:_fP_(nonnull_id__MTLDevice__)_device(_fBMPSCNNLossDescriptor_fP_*_Nonnull)_lossDescriptor"><a class="permalink" href="#_-_(nonnull_instancetype)__fBinitWithDevice:_fP_(nonnull_id__MTLDevice__)_device(_fBMPSCNNLossDescriptor_fP_*_Nonnull)_lossDescriptor">-
  (nonnull instancetype) <b>initWithDevice:</b> (nonnull id&lt; MTLDevice &gt;)
  device(<b>MPSCNNLossDescriptor</b> *_Nonnull) lossDescriptor</a></h2>
Initialize the loss filter with a loss descriptor.
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The device the filter will run on.
<br/>
<i>lossDescriptor</i> The loss descriptor.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> valid <b>MPSCNNLoss</b> object or nil, if
  failure.</div>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Property_Documentation"><a class="permalink" href="#Property_Documentation">Property
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss" id="_-_(float)_delta_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP"><a class="permalink" href="#_-_(float)_delta_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP">-
  (float) delta [read]<b>, [nonatomic]</b>, [assign]<b></b></a></h2>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(float)_epsilon_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP"><a class="permalink" href="#_-_(float)_epsilon_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP">-
  (float) epsilon [read]<b>, [nonatomic]</b>, [assign]<b></b></a></h2>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(float)_labelSmoothing_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP"><a class="permalink" href="#_-_(float)_labelSmoothing_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP">-
  (float) labelSmoothing [read]<b>, [nonatomic]</b>, [assign]<b></b></a></h2>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(_fBMPSCNNLossType_fP)_lossType_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP"><a class="permalink" href="#_-_(_fBMPSCNNLossType_fP)_lossType_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP">-
  (<b>MPSCNNLossType</b>) lossType [read]<b>, [nonatomic]</b>,
  [assign]<b></b></a></h2>
See <b>MPSCNNLossDescriptor</b> for information about the following properties.
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(NSUInteger)_numberOfClasses_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP"><a class="permalink" href="#_-_(NSUInteger)_numberOfClasses_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP">-
  (NSUInteger) numberOfClasses [read]<b>, [nonatomic]</b>,
  [assign]<b></b></a></h2>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(_fBMPSCNNReductionType_fP)_reductionType_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP"><a class="permalink" href="#_-_(_fBMPSCNNReductionType_fP)_reductionType_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP">-
  (<b>MPSCNNReductionType</b>) reductionType [read]<b>, [nonatomic]</b>,
  [assign]<b></b></a></h2>
</section>
<section class="Ss">
<h2 class="Ss" id="_-_(float)_weight_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP"><a class="permalink" href="#_-_(float)_weight_fC__read__fP,__fC__nonatomic__fP,__fC__assign__fP">-
  (float) weight [read]<b>, [nonatomic]</b>, [assign]<b></b></a></h2>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Author"><a class="permalink" href="#Author">Author</a></h1>
Generated automatically by Doxygen for MetalPerformanceShaders.framework from
  the source code.
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">Mon Jul 9 2018</td>
    <td class="foot-os">Version MetalPerformanceShaders-119.3</td>
  </tr>
</table>
</body>
</html>
