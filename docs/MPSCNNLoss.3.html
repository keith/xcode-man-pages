<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   -*- nroff -*-
 -->
<head>

<style>
@media (prefers-color-scheme: dark) {
  body {
    background: #000;
    color: #d0d0d0;
  }

  a, a:visited {
    color: #1899eb;
  }
}
</style>

  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    .Nd, .Bf, .Op { display: inline; }
    .Pa, .Ad { font-style: italic; }
    .Ms { font-weight: bold; }
    .Bl-diag > dt { font-weight: bold; }
    code.Nm, .Fl, .Cm, .Ic, code.In, .Fd, .Fn, .Cd { font-weight: bold;
      font-family: inherit; }
  </style>
  <title>MPSCNNLoss(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">MPSCNNLoss(3)</td>
    <td class="head-vol">MetalPerformanceShaders.framework</td>
    <td class="head-rtitle">MPSCNNLoss(3)</td>
  </tr>
</table>
<div class="manual-text">
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">MPSCNNLoss</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<p class="Pp">#import &lt;MPSCNNLoss.h&gt;</p>
<p class="Pp">Inherits <b>MPSCNNKernel</b>.</p>
<section class="Ss">
<h2 class="Ss" id="Instance_Methods"><a class="permalink" href="#Instance_Methods">Instance
  Methods</a></h2>
<br/>
<p class="Pp">(nonnull instancetype) - <b>initWithDevice:</b>
  <br/>
  (nonnull instancetype) - <b>initWithDevice:lossDescriptor:</b>
  <br/>
  (nullable instancetype) - <b>initWithCoder:device:</b>
  <br/>
  (void) - <b>encodeToCommandBuffer:sourceImage:labels:destinationImage:</b>
  <br/>
  (<b>MPSImage</b> *__nonnull) -
    <b>encodeToCommandBuffer:sourceImage:labels:</b>
  <br/>
  (void) -
    <b>encodeBatchToCommandBuffer:sourceImages:labels:destinationImages:</b>
  <br/>
  (<b>MPSImageBatch</b> *__nonnull) -
    <b>encodeBatchToCommandBuffer:sourceImages:labels:</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Properties"><a class="permalink" href="#Properties">Properties</a></h2>
<br/>
<p class="Pp"><b>MPSCNNLossType</b> <b>lossType</b>
  <br/>
  <b>MPSCNNReductionType</b> <b>reductionType</b>
  <br/>
  float <b>weight</b>
  <br/>
  float <b>labelSmoothing</b>
  <br/>
  NSUInteger <b>numberOfClasses</b>
  <br/>
  float <b>epsilon</b>
  <br/>
  float <b>delta</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Additional_Inherited_Members"><a class="permalink" href="#Additional_Inherited_Members">Additional
  Inherited Members</a></h2>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Detailed_Description"><a class="permalink" href="#Detailed_Description">Detailed
  Description</a></h1>
<p class="Pp">This depends on Metal.framework. The <b>MPSCNNLoss</b> filter is
    only used for training. This filter performs both the forward and backward
    pass computations. Specifically, it computes the loss between the input
    (predictions) and target data (labels) and the loss gradient. The loss value
    can be a 1 x 1 x 1 image containing a scalar loss value or an image (of the
    same size as the input source image) with per feature channel losses. The
    loss value is used to determine whether to continue the training operation
    or to terminate it, once satisfactory results are achieved. The loss
    gradient is the first gradient computed for the backward pass and serves as
    input to the next gradient filter (in the backward direction).</p>
<p class="Pp">The <b>MPSCNNLoss</b> filter is created with a
    <b>MPSCNNLossDescriptor</b> describing the type of a loss filter and the
    type of a reduction to use for computing the overall loss.</p>
<p class="Pp">The <b>MPSCNNLoss</b> filter takes the output of the inference
    pass (predictions) as input. It also requires the target data (labels) and
    optionally, weights for the labels. If per-label weights are not supplied,
    there is an option to use a single weight value by setting the 'weight'
    properly on the <b>MPSCNNLossDescriptor</b> object. The labels and optional
    weights need to be supplied by the user using the <b>MPSCNNLossLabels</b>
    object. The labels and weights are described via the
    <b>MPSCNNLossDataDescriptor</b> objects, which are in turn used to
    initialize the <b>MPSCNNLossLabels</b> object.</p>
<p class="Pp">If the specified reduction operation is MPSCNNReductionTypeNone,
    the destinationImage should be at least as large as the specified clipRect.
    The detinationImage will then contain per-element losses. Otherse, a
    reduction operation will be performed, according to the specified reduction
    type, and the filter will return a scalar value containing the overall loss.
    For more information on the available reduction types, see
    <b>MPSCNNTypes.h</b>. Also see <b>MPSCNNLossDescriptor</b> for the
    description of optional parameters.</p>
<p class="Pp">Here is a code example:</p>
<p class="Pp">// Setup MPSCNNLossDataDescriptor* labelsDescriptor =
    [<b>MPSCNNLossDataDescriptor</b> cnnLossDataDescriptorWithData: labelsData
    layout: MPSDataLayoutHeightxWidthxFeatureChannels size: labelsDataSize];
    MPSCNNLossLabels* labels = [[<b>MPSCNNLossLabels</b> alloc] initWithDevice:
    device labelsDescriptor: labelsDescriptor]; <b>MPSCNNLossDescriptor</b>
    <i>lossDescriptor = [</i><b>MPSCNNLossDescriptor</b><i>
    cnnLossDescriptorWithType: (MPSCNNLossType)MPSCNNLossTypeMeanAbsoluteError
    reductionType: (MPSCNNReductionType)MPSCNNReductionTypeSum];
    </i><b>MPSCNNLoss</b><i></i><b> lossFilter = [[</b><b>MPSCNNLoss</b><b>
    alloc] initWithDevice: device lossDescriptor: lossDescriptor];</b></p>
<p class="Pp">// Encode loss filter. // The sourceImage is the output of a
    previous layer, for example, the SoftMax layer. The lossGradientsImage // is
    the sourceGradient input image to the first gradient layer (in the backward
    direction), for example, // the SoftMax gradient filter. [lossFilter
    encodeToCommandBuffer: commandBuffer sourceImage: sourceImage labels: labels
    destinationImage: lossGradientsImage];</p>
<p class="Pp">// In order to gaurantee that the loss image data is correctly
    synchronized for CPU side access, // it is the application's responsibility
    to call the [labels synchronizeOnCommandBuffer:] // method before accessing
    the loss image data. [labels synchronizeOnCommandBuffer:commandBuffer];
    MPSImage* lossImage = [labels lossImage];</p>
<p class="Pp"></p>
<pre>
<br/>
        For predictions (y) and labels (t), the available loss filter types are the following:
<br/>
        Mean Absolute Error loss filter. This filter measures the absolute error of the element-wise
<br/>
        difference between the predictions and labels.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = |y - t|
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        Mean Squared Error loss filter. This filter measures the squared error of the element-wise
<br/>
        difference between the predictions and labels.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = (y - t)^2
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        SoftMax Cross Entropy loss filter. This loss filter is applied element-wise.
<br/>
        This loss filter combines the LogSoftMax and Negative Log Likelihood operations in a
<br/>
        single filter. It is useful for training a classification problem with multiple classes.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = -t * LogSoftMax(y)
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
                                     If reductionType is MPSCNNReductionTypeMean, the accumulated
<br/>
                                     loss value is divided by width * height instead of
<br/>
                                     width * height * featureChannels.
<br/>
        Sigmoid Cross Entropy loss filter. This loss filter is applied element-wise.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = max(y, 0) - y * t + log(1 + exp(-|y|))
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        Categorical Cross Entropy loss filter. This loss filter is applied element-wise.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = -t * log(y)
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        Hinge loss filter. This loss filter is applied element-wise.
<br/>
        The labels are expected to be 0.0 or 1.0.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = max(1 - (t * y), 0.0f)
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        Huber loss filter. This loss filter is applied element-wise.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          if (|y - t| &lt;= delta, losses = 0.5 * y^2
<br/>
                                     if (|y - t| &gt;  delta, losses = 0.5 * delta^2 + delta * (|y - t| - delta)
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        Cosine Distance loss filter. This loss filter is applied element-wise.
<br/>
        The only valid reduction type for this loss filter is MPSCNNReductionTypeSum.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          loss = 1 - reduce_sum(y * t)
<br/>
            Compute overall loss:    weighted_loss = weight * loss
<br/>
        Log loss filter. This loss filter is applied element-wise.
<br/>
        This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = -(t * log(y + epsilon)) - ((1 - t) * log(1 - y + epsilon))
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        Kullback-Leibler Divergence loss filter. This loss filter is applied element-wise.
<br/>
        The input (predictions) is expected to contain log-probabilities.
<br/>
            This loss function is computed according to the following formulas:
<br/>
            Compute losses:          losses = t * (log(t) - y)
<br/>
            Compute weighted losses: weighted_losses = weight(s) * losses
<br/>
            Compute overall loss:    loss = reduce(weighted_losses, reductionType)
<br/>
        For predictions (y) and labels (t), the loss gradient for each available loss filter type
<br/>
        is computed as follows:
<br/>
        Mean Absolute Error loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = (y - t) / |y - t|
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Mean Squared Error loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = 2 * (y - t)
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        SoftMax Cross Entropy loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            First, apply the same label smoothing as in the MPSCNNLoss filter.
<br/>
            Compute gradient:          d/dy = y - t
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Sigmoid Cross Entropy loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
        First, apply the same label smoothing as in the MPSCNNLoss filter.
<br/>
            Compute gradient:          d/dy = (1 / (1 + exp(-y)) - t
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Categorical Cross Entropy loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = -t / y
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Hinge loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = ((1 + ((1 - (2 * t)) * y)) &gt; 0) ? 1 - (2 * t) : 0
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Huber loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = |y - t| &gt; delta ? delta : y - t
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Cosine Distance loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = -t
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Log loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = (-2 * epsilon * t - t + y + epsilon) / (y * (1 - y) + epsilon * (epsilon + 1))
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        Kullback-Leibler Divergence loss.
<br/>
        The loss gradient is computed according to the following formulas:
<br/>
            Compute gradient:          d/dy = -t / y
<br/>
            Compute weighted gradient: weighted_gradient = weight(s) * gradient
<br/>
        The number of output feature channels remains the same as the number of input feature
<br/>
        channels..fi</pre>
<pre>
<br/>
 </pre>
</section>
<section class="Sh">
<h1 class="Sh" id="Method_Documentation"><a class="permalink" href="#Method_Documentation">Method
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImageBatch</b>*__nonnull) encodeBatchToCommandBuffer:
  (nonnull id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b>
  *__nonnull) sourceImage(<b>MPSCNNLossLabelsBatch</b> *__nonnull) labels</h2>
</section>
<section class="Ss">
<h2 class="Ss">- (void) encodeBatchToCommandBuffer: (nonnull id&lt;
  MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b> *__nonnull)
  sourceImage(<b>MPSCNNLossLabelsBatch</b> *__nonnull)
  labels(<b>MPSImageBatch</b> *__nonnull) destinationImage</h2>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImage</b>*__nonnull) encodeToCommandBuffer: (nonnull
  id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImage</b> *__nonnull)
  sourceImage(<b>MPSCNNLossLabels</b> *__nonnull) labels</h2>
<p class="Pp">Encode a <b>MPSCNNLoss</b> filter and return a gradient. This
    -encode call is similar to the
    encodeToCommandBuffer:sourceImage:labels:destinationImage: above, except
    that it creates and returns the <b>MPSImage</b> with the loss gradient
    result.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The MTLCommandBuffer on which to
  encode.
<br/>
<i>sourceImage</i> The source image from the previous filter in the graph (in
  the inference direction).
<br/>
<i>labels</i> The object containing the target data (labels) and optionally,
  weights for the labels.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">The <b>MPSImage</b> containing the gradient result.</div>
</section>
<section class="Ss">
<h2 class="Ss">- (void) encodeToCommandBuffer: (nonnull id&lt; MTLCommandBuffer
  &gt;) commandBuffer(<b>MPSImage</b> *__nonnull)
  sourceImage(<b>MPSCNNLossLabels</b> *__nonnull) labels(<b>MPSImage</b>
  *__nonnull) destinationImage</h2>
<p class="Pp">Encode a <b>MPSCNNLoss</b> filter and return a gradient in the
    destinationImage. This filter consumes the output of a previous layer, for
    example, the SoftMax layer containing predictions, and the
    <b>MPSCNNLossLabels</b> object containing the target data (labels) and
    optionally, weights for the labels. The destinationImage contains the
    computed gradient for the loss layer. It serves as a source gradient input
    image to the first gradient layer (in the backward direction), in our
    example, the SoftMax gradient layer.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The MTLCommandBuffer on which to
  encode.
<br/>
<i>sourceImage</i> The source image from the previous filter in the graph (in
  the inference direction).
<br/>
<i>labels</i> The object containing the target data (labels) and optionally,
  weights for the labels.
<br/>
<i>destinationImage</i> The <b>MPSImage</b> into which to write the gradient
  result.</div>
</section>
<section class="Ss">
<h2 class="Ss">- (nullable instancetype) <b>initWithCoder:</b> (NSCoder
  *__nonnull) aDecoder(nonnull id&lt; MTLDevice &gt;) device</h2>
<p class="Pp">&lt;NSSecureCoding&gt; support</p>
<p class="Pp">Reimplemented from <b>MPSCNNKernel</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype) initWithDevice: (nonnull id&lt;
  MTLDevice &gt;) device</h2>
<p class="Pp">Standard init with default properties per filter type</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The device that the filter will be used on.
  May not be NULL.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> pointer to the newly initialized object. This
  will fail, returning nil if the device is not supported. Devices must be
  MTLFeatureSet_iOS_GPUFamily2_v1 or later.</div>
<p class="Pp">Reimplemented from <b>MPSCNNKernel</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype) <b>initWithDevice:</b> (nonnull id&lt;
  MTLDevice &gt;) device(<b>MPSCNNLossDescriptor</b> *_Nonnull)
  lossDescriptor</h2>
<p class="Pp">Initialize the loss filter with a loss descriptor.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The device the filter will run on.
<br/>
<i>lossDescriptor</i> The loss descriptor.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> valid <b>MPSCNNLoss</b> object or nil, if
  failure.</div>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Property_Documentation"><a class="permalink" href="#Property_Documentation">Property
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (float) delta [read]<b>, [nonatomic]</b>, [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (float) epsilon [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (float) labelSmoothing [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSCNNLossType</b>) lossType [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">See <b>MPSCNNLossDescriptor</b> for information about the
    following properties.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) numberOfClasses [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSCNNReductionType</b>) reductionType [read]<b>,
  [nonatomic]</b>, [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (float) weight [read]<b>, [nonatomic]</b>, [assign]<b></b></h2>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Author"><a class="permalink" href="#Author">Author</a></h1>
<p class="Pp">Generated automatically by Doxygen for
    MetalPerformanceShaders.framework from the source code.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">Mon Jul 9 2018</td>
    <td class="foot-os">Version MetalPerformanceShaders-119.3</td>
  </tr>
</table>
</body>
</html>
