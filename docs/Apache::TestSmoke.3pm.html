<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   Automatically generated by Pod::Man 2.27 (Pod::Simple 3.28)
  
   Standard preamble:
   ========================================================================
   Vertical space (when we can't use .PP)
   Begin verbatim text
   End verbatim text
   Set up some character translations and predefined strings.  \*(-- will
   give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
   double quote, and \*(R" will give a right double quote.  \*(C+ will
   give a nicer C++.  Capital omega is used to do unbreakable dashes and
   therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
   nothing in troff, for use with C<>.
   diablo 10 pitch
    diablo 12 pitch
 -->
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
    div.Nd, div.Bf, div.Op { display: inline; }
    span.Pa, span.Ad { font-style: italic; }
    span.Ms { font-weight: bold; }
    dl.Bl-diag > dt { font-weight: bold; }
    code.Nm, code.Fl, code.Cm, code.Ic, code.In, code.Fd, code.Fn,
    code.Cd { font-weight: bold; font-family: inherit; }
  </style>
  <title>Apache::TestSmoke(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">Apache::TestSmoke(3)</td>
    <td class="head-vol">User Contributed Perl Documentation</td>
    <td class="head-rtitle">Apache::TestSmoke(3)</td>
  </tr>
</table>
<div class="manual-text">
<br/>
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
Apache::TestSmoke - Special Tests Sequence Failure Finder
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<span class="Li"></span>
<pre>
  # get the usage and the default values
  % t/SMOKE -help

  # repeat all tests 5 times and save the report into
  # the file 'myreport'
  % t/SMOKE -times=5 -report=myreport

  # run all tests default number of iterations, and repeat tests
  # default number of times
  % t/SMOKE

  # same as above but work only the specified tests
  % t/SMOKE foo/bar foo/tar

  # run once a sequence of tests in a non-random mode
  # e.g. when trying to reduce a known long sequence that fails
  % t/SMOKE -order=rotate -times=1 foo/bar foo/tar

  # show me each currently running test
  # it's not the same as running the tests in the verbose mode
  % t/SMOKE -verbose

  # run t/TEST, but show any problems after *each* tests is run
  # useful for bug reports (it actually runs t/TEST -start, then
  # t/TEST -run for each test separately and finally t/TEST -stop
  % t/SMOKE -bug_mode

  # now read the created report file
</pre>
</section>
<section class="Sh">
<h1 class="Sh" id="DESCRIPTION"><a class="permalink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<section class="Ss">
<h2 class="Ss" id="The_Problem"><a class="permalink" href="#The_Problem">The
  Problem</a></h2>
When we try to test a stateless machine (i.e. all tests are independent),
  running all tests once ensures that all tested things properly work. However
  when a state machine is tested (i.e. where a run of one test may influence
  another test) it's not enough to run all the tests once to know that the
  tested features actually work. It's quite possible that if the same tests are
  run in a different order and/or repeated a few times, some tests may fail.
  This usually happens when some tests don't restore the system under test to
  its pristine state at the end of the run, which may influence other tests
  which rely on the fact that they start on pristine state, when in fact it's
  not true anymore. In fact it's possible that a single test may fail when run
  twice or three times in a sequence.
</section>
<section class="Ss">
<h2 class="Ss" id="The_Solution"><a class="permalink" href="#The_Solution">The
  Solution</a></h2>
To reduce the possibility of such dependency errors, it's helpful to run random
  testing repeated many times with many different srand seeds. Of course if no
  failures get spotted that doesn't mean that there are no tests
  inter-dependencies, which may cause a failure in production. But random
  testing definitely helps to spot many problems and can give better test
  coverage.
</section>
<section class="Ss">
<h2 class="Ss" id="Resolving_Sequence_Problems"><a class="permalink" href="#Resolving_Sequence_Problems">Resolving
  Sequence Problems</a></h2>
When this kind of testing is used and a failure is detected there are two
  problems:
<dl class="Bl-tag">
  <dt>1.</dt>
  <dd>First is to be able to reproduce the problem so if we think we fixed it,
      we could verify the fix. This one is easy, just remember the sequence of
      tests run till the failed test and rerun the same sequence once again
      after the problem has been fixed.</dd>
  <dt>2.</dt>
  <dd>Second is to be able to understand the cause of the problem. If during the
      random test the failure has happened after running 400 tests, how can we
      possibly know which previously running tests has caused to the failure of
      the test 401. Chances are that most of the tests were clean and don't have
      inter-dependency problem. Therefore it'd be very helpful if we could
      reduce the long sequence to a minimum. Preferably 1 or 2 tests. That's
      when we can try to understand the cause of the detected problem.</dd>
</dl>
<p class="Pp">This utility attempts to solve both problems, and at the end of
    each iteration print a minimal sequence of tests causing to a failure. This
    doesn't always succeed, but works in many cases.</p>
<p class="Pp">This utility:</p>
<dl class="Bl-tag">
  <dt>1.</dt>
  <dd>Runs the tests randomly until the first failure is detected. Or
      non-randomly if the option <i>-order</i> is set to <i>repeat</i> or
      <i>rotate</i>.</dd>
  <dt>2.</dt>
  <dd>Then it tries to reduce that sequence of tests to a minimum, and this
      sequence still causes to the same failure.</dd>
  <dt>3.</dt>
  <dd>(XXX: todo): then it reruns the minimal sequence in the verbose mode and
      saves the output.</dd>
  <dt>4.</dt>
  <dd>It reports all the successful reductions as it goes to STDOUT and report
      file of the format: smoke-report-&lt;date&gt;.txt.
    <p class="Pp">In addition the systems build parameters are logged into the
        report file, so the detected problems could be reproduced.</p>
  </dd>
  <dt>5.</dt>
  <dd>Goto 1 and run again using a new random seed, which potentially should
      detect different failures.</dd>
</dl>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Reduction_Algorithm"><a class="permalink" href="#Reduction_Algorithm">Reduction
  Algorithm</a></h1>
Currently for each reduction path, the following reduction algorithms get
  applied:
<dl class="Bl-tag">
  <dt>1.</dt>
  <dd>Binary search: first try the upper half then the lower.</dd>
  <dt>2.</dt>
  <dd>Random window: randomize the left item, then the right item and return the
      items between these two points.</dd>
</dl>
</section>
<section class="Sh">
<h1 class="Sh" id="t/SMOKE.PL"><a class="permalink" href="#t/SMOKE.PL">t/SMOKE.PL</a></h1>
<i>t/SMOKE.PL</i> is driving this module, if you don't have it, create it:
<p class="Pp"><span class="Li"></span></p>
<pre>
  #!perl

  use strict;
  use warnings FATAL =&gt; 'all';

  use FindBin;
  use lib &quot;$FindBin::Bin/../Apache-Test/lib&quot;;
  use lib &quot;$FindBin::Bin/../lib&quot;;

  use Apache::TestSmoke ();

  Apache::TestSmoke-&gt;new(@ARGV)-&gt;run;
</pre>
<p class="Pp">usually <i>Makefile.PL</i> converts it into <i>t/SMOKE</i> while
    adjusting the perl path, but you create <i>t/SMOKE</i> in first place as
    well.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="AUTHOR"><a class="permalink" href="#AUTHOR">AUTHOR</a></h1>
Stas Bekman
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">2015-06-18</td>
    <td class="foot-os">perl v5.18.2</td>
  </tr>
</table>
</body>
</html>
